[["Map",1,2,9,10,114,115],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.16.6","content-config-digest","fe2a808538ebf6a5","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://clement.champouillon.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"assets\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{\"yml\":\"yaml\"},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","cv",["Map",11,12,31,32,43,44,56,57,72,73,88,89,103,104],"1",{"id":11,"data":13,"filePath":28,"assetImports":29},{"company":14,"position":15,"location":16,"start":17,"end":18,"description":19,"keypoints":20,"logo":27},"Bedrock Streaming","Frontent developer","Lyon, France",["Date","2019-02-01T00:00:00.000Z"],["Date","2020-02-01T00:00:00.000Z"],"Developed streaming applications for HBBTV, OTT and TV related platforms.",[21,22,23,24,25,26],"HBBTV application development","React frontend development","Video player integration, with support for DRM and ad insertion","Cross-platform compatibility","DVB standards compliance","Unit, integration, and end-to-end testing","__ASTRO_IMAGE_../assets/bedrock_streaming_logo.jpeg","src/data/cv.json",[30],"../assets/bedrock_streaming_logo.jpeg","2",{"id":31,"data":33,"filePath":28,"assetImports":42},{"company":14,"position":34,"location":16,"start":35,"end":36,"description":37,"keypoints":38,"logo":27},"Lead frontend developer",["Date","2020-02-01T00:00:00.000Z"],["Date","2021-11-01T00:00:00.000Z"],"Led the frontend development team, focusing on HBBTV and OTT applications.",[39,40,41],"Team leadership","Leading a full remote team","Plus all keypoints from previous position",[30],"3",{"id":43,"data":45,"filePath":28,"assetImports":54},{"company":46,"position":47,"location":16,"start":48,"end":49,"description":50,"keypoints":51,"logo":53},"LinkValue","Frontend developer",["Date","2021-11-01T00:00:00.000Z"],["Date","2022-07-01T00:00:00.000Z"],"Development of web applications for businesses like selling platforms",[52],"React and TypeScript development","__ASTRO_IMAGE_../assets/linkvalue_logo.jpeg",[55],"../assets/linkvalue_logo.jpeg","4",{"id":56,"data":58,"filePath":28,"assetImports":70},{"company":59,"position":60,"location":16,"start":61,"end":62,"description":63,"keypoints":64,"logo":69},"Euronews","Fullstack developer",["Date","2022-07-01T00:00:00.000Z"],["Date","2022-12-01T00:00:00.000Z"],"Euronews is a European news media company that provides wide range of news content in 16 languages.",[65,66,67,68],"Synfony PHP development","Frontend integration","Third parties integration","Youtube, Dailymotion and JWPlayer integration","__ASTRO_IMAGE_../assets/euronews_logo.svg",[71],"../assets/euronews_logo.svg","5",{"id":72,"data":74,"filePath":28,"assetImports":86},{"company":75,"position":76,"location":77,"start":78,"end":79,"keypoints":80,"logo":85},"Sparted","Chief technology officer","Paris, France",["Date","2023-02-01T00:00:00.000Z"],["Date","2024-06-01T00:00:00.000Z"],[81,82,83,84],"Key decision maker for the technology stack","Leading the development team","React Native development and backend with NestJS","Cloud infrastructure management with AWS and k8s","__ASTRO_IMAGE_../assets/sparted_logo.jpeg",[87],"../assets/sparted_logo.jpeg","6",{"id":88,"data":90,"filePath":28,"assetImports":101},{"company":91,"position":92,"location":77,"start":93,"description":94,"keypoints":95,"logo":100},"Hivebrite","Mobile developer",["Date","2024-07-01T00:00:00.000Z"],"Hivebrite is a community management platform that helps organizations or universities to build, manage and engage their peers.",[96,97,98,99],"React Native development","Cross-platform mobile/web development","Integration with backend services","Performance optimization","__ASTRO_IMAGE_../assets/hivebrite_logo.jpeg",[102],"../assets/hivebrite_logo.jpeg","7",{"id":103,"data":105,"filePath":28,"assetImports":112},{"company":106,"position":76,"location":16,"start":107,"description":108,"keypoints":109,"logo":111},"Komia",["Date","2024-07-01T00:00:00.000Z"],"Komia provides all-in-one software solutions for restaurants and hotels",[96,110],"Google Cloud Platform","__ASTRO_IMAGE_../assets/komia.png",[113],"../assets/komia.png","blog",["Map",116,117],"mastra",{"id":116,"data":118,"body":122,"filePath":123,"assetImports":124,"digest":126,"rendered":127},{"title":119,"date":120,"cover":121},"Mastra, how a TypeScript AI framework clarified agents for me",["Date","2025-12-30T00:00:00.000Z"],"__ASTRO_IMAGE_./mastra.png","I discovered Mastra through my GitHub feed, a reminder of how curated technical networks surface relevant tools without noise.\nFor developers, this is where real discovery happens: raw signals from peers, free of marketing fluff. It's just facts.\n\n## What caught my attention\n\n> Build agents with a modern TypeScript stack\n\n_â˜ï¸ Mastra's homepage headline_\n\nThe most obvious is the keyword **agents**, **AI agents**. Everybody wants to be onboard the AI train, including myself, and although I\nam carefully trying not to fall into the trend trap, I also want to learn more about what is effectively becoming the\nbiggest revolution since Internet was invented.\n\nBesides AI, it's **Typescript**. I am myself coming from a frontend developer background, learned Typescript, and still\nto date use it everyday. What's obviously convinient when you become an expert with one programing language is that you\nalmost don't have to think to actually program what you have in mind. And that becomes very convinient when you are\ngetting familiar with a new topic. Forget about the syntax, use the appropriate tooling, refactor in no time...\n\nLast but not least, and because I've first seen it in my GitHub feed: it's **open-source**. I love open-source. I bet you\ntoo.\n\n## Getting started, it takes literally 2 minutes\n\nInitialising a mastra codebase has been made super easy with just a starter command, plus a bunch of templates you can\nchoose from!\n\nWith this command executed, you get a pretty simple folder architecture, and some key files every Typescript developer\nis already familiar with: `package.json`, `tsconfig.json`, `.env.example`...\n\n## Discovering Studio UI\n\nStudio UI is an amazing frontend application that comes with Mastra. It's representing every entity you built in your\ncodebase, like a **tool**, an **agent**, a **workflow**, and even the infrastucture connections you made.\n\nI started easy by just creating an agent with no capabilities but chat with me. I used a Mistral API key, followed the\nMastra documentation to configure the bare minimum agent, and the magic happened âœ¨. Using Studio, I was able to create\na new conversation with my agent. Immediately after my conversation I started to **customize the \"system prompt\"** of my\nagent, meaning the initial instructions my agent is given before any conversation starts.\n\n**At this point, just 10 minutes before starting, it was already fun to play around with Mastra.**\n\nOkay that's cool already. But let's get our hands dirty.\n\n## Chaining entities\n\nPretty much any well-known AI provider today is offering a web UI to chat with their model, but also to create custom agents,\nconnect some mainstream tools like your calendar, your emails, your source code or whatsoever.\n\n**By the way, quick parenthesis here as a kindly reminder that today's free or individual plans offered by big AI companies\nfor an access to a web interface to chat and play around with models are well underpriced. That means, they\nbenefit from your daily usage of such tool (your data, the feedbacks you make...).**\n\nDisclaimer done. Now what brings Mastra if you can already create agents only? I continued my exploration by **creating a tool**.\nA tool is a interface to and from another program. **It's basically a side effect**. There are input and output data to almost any\ntool, validated by a schema. I decided to create tools to interact with a backend of a todo list sandbox project, so that my agent\ncan manage for me this list.\n\nThe first tool is to fetch all the remaining todos for the day, the second to mark one task as complete, and a last one to create\nnew elements in my list. Now that my tools are ready, I can connect them to my agent, and with a properly tuned system prompt, my agent would know\nexactly when to use those tools.\n\nAnd you get immediate results! ðŸ¤¯\n\nWith Studio, it's super easy to test your tools independently, or together, interact with your agent and verify it's calling the appropriate\ntools, see the input and output of each call and so on!\n\nFor complex tasks involving multiple tools or agents, you can create workflows. Workflows basically allow to have one tool (or agent) called\nafter another, which each step responsible for a defined task.\n\n## Connect infrastructure\n\nYou can add super-powers to your AI just by giving it some infrastructure resources. Within minutes you can give your agents a memory, that means\nthat your agent will remember previous messages in the conversation, to potentially fine-tune the upcoming answers. If you add another adapter,\nyou can get your agent to share a memory across several conversations.\n\nOverall many of the advanced features (e.g., memory, RAG) of Mastra require you connect a database, so make yourself a favor and write a small `compose.yml` file\nso you can have a confined database ready to serve your local environnement.\n\n```yaml\nservices:\n  db:\n    image: postgres:16\n    ports:\n      - \"5432:5432\"\n```\n\n## I understood RAG (Retrieval-Augmented Generation)\n\nNow that I'm completely trapped into the rabbit hole, I'm curious to test some of the unknown and most advanced features. I went back to the\ndocumentation, clicked a random tab in the side navigation panel and picked... RAG.\n\nI first read a bit of theory about this topic to understand what this is all about before testing it. I will try to make rapid explaination here:\nRAG uses non-specialist AI models to answer very specific questions using additionnal knowledge taken from resources like text documents.\nThe benefits of this technique is that your AI will be able to answer based on documents that have never been used to train public AI models, let's say\nfor example your product documentation.\n\nit's a multiple steps process that involves **parsing some documents**, **splitting it**, **embedding it**, **storing vectors** and **make them accessible** to an LLM.\nIn other terms, that means:\n\n- Read some input documents with an appropriate parser (let's say your documentation in markdown)\n- Make small chunks out of it (can be a group of words, a number of letter, a paragraph...)\n- Transform chunks into vectors: that's called embeddings\n- Store the vector into a vector database (if you like postgres, there is a pgVector adapter)\n- Make the database accessible to your agents\n\nI tried this with the documentation of an old project I work on, and the result felt exactly like the support chatbots you sometimes stumble across online. Except that\nmy agent felt way more talented... ðŸ˜œ\n\n## Road to production\n\nNow the obvious question that comes after days of experimenting with the codebase and Studio is: how do I host this thing and how do I connect it to my project?\nOnce more Mastra got you covered with a bunch of options to choose from:\n\n- Mastra Cloud (the commercial offer)\n- Integration with another framework like Next.js or Astro\n- Serverless deployment\n- Custom server deployment\n\nAbout this last option: Mastra provides a ready to use HTTP API that allows to call each one of the tools, agents, workflows - and generally spoken entities - you created in\nyour Mastra codebase. That means you can from your mobile app for example with just one HTTP call talk to your custom agent. And to make things even simpler, if your\nfrontend codebase is in Typescript, they provide a client to simplify interactions with the server.\n\n## Why you should try\n\nIf like me you don't want to lag behind the market and want to strengthen your AI understanding and skillset, it's a perfect sandbox. There is a whole bunch of\nuse-cases that emerge from the experiments you can do with Mastra. **It's a fun way of learning**.\n\nAnd if you accidentally create the perfect solution to a problem with this framework (you might), then you will be lucky enough to have a production ready solution,\nbacked by talented open-source contributors: the team behind Mastra is also responsible for the success of Gatsby, the frontend framework.\n\n**Stay tuned, Mastra 1.0 is going live in January 2026**","src/content/blog/mastra.md",[125],"./mastra.png","2d218b92a2707a18",{"html":128,"metadata":129},"\u003Cp>I discovered Mastra through my GitHub feed, a reminder of how curated technical networks surface relevant tools without noise.\nFor developers, this is where real discovery happens: raw signals from peers, free of marketing fluff. Itâ€™s just facts.\u003C/p>\n\u003Ch2 id=\"what-caught-my-attention\">What caught my attention\u003C/h2>\n\u003Cblockquote>\n\u003Cp>Build agents with a modern TypeScript stack\u003C/p>\n\u003C/blockquote>\n\u003Cp>\u003Cem>â˜ï¸ Mastraâ€™s homepage headline\u003C/em>\u003C/p>\n\u003Cp>The most obvious is the keyword \u003Cstrong>agents\u003C/strong>, \u003Cstrong>AI agents\u003C/strong>. Everybody wants to be onboard the AI train, including myself, and although I\nam carefully trying not to fall into the trend trap, I also want to learn more about what is effectively becoming the\nbiggest revolution since Internet was invented.\u003C/p>\n\u003Cp>Besides AI, itâ€™s \u003Cstrong>Typescript\u003C/strong>. I am myself coming from a frontend developer background, learned Typescript, and still\nto date use it everyday. Whatâ€™s obviously convinient when you become an expert with one programing language is that you\nalmost donâ€™t have to think to actually program what you have in mind. And that becomes very convinient when you are\ngetting familiar with a new topic. Forget about the syntax, use the appropriate tooling, refactor in no timeâ€¦\u003C/p>\n\u003Cp>Last but not least, and because Iâ€™ve first seen it in my GitHub feed: itâ€™s \u003Cstrong>open-source\u003C/strong>. I love open-source. I bet you\ntoo.\u003C/p>\n\u003Ch2 id=\"getting-started-it-takes-literally-2-minutes\">Getting started, it takes literally 2 minutes\u003C/h2>\n\u003Cp>Initialising a mastra codebase has been made super easy with just a starter command, plus a bunch of templates you can\nchoose from!\u003C/p>\n\u003Cp>With this command executed, you get a pretty simple folder architecture, and some key files every Typescript developer\nis already familiar with: \u003Ccode>package.json\u003C/code>, \u003Ccode>tsconfig.json\u003C/code>, \u003Ccode>.env.example\u003C/code>â€¦\u003C/p>\n\u003Ch2 id=\"discovering-studio-ui\">Discovering Studio UI\u003C/h2>\n\u003Cp>Studio UI is an amazing frontend application that comes with Mastra. Itâ€™s representing every entity you built in your\ncodebase, like a \u003Cstrong>tool\u003C/strong>, an \u003Cstrong>agent\u003C/strong>, a \u003Cstrong>workflow\u003C/strong>, and even the infrastucture connections you made.\u003C/p>\n\u003Cp>I started easy by just creating an agent with no capabilities but chat with me. I used a Mistral API key, followed the\nMastra documentation to configure the bare minimum agent, and the magic happened âœ¨. Using Studio, I was able to create\na new conversation with my agent. Immediately after my conversation I started to \u003Cstrong>customize the â€œsystem promptâ€\u003C/strong> of my\nagent, meaning the initial instructions my agent is given before any conversation starts.\u003C/p>\n\u003Cp>\u003Cstrong>At this point, just 10 minutes before starting, it was already fun to play around with Mastra.\u003C/strong>\u003C/p>\n\u003Cp>Okay thatâ€™s cool already. But letâ€™s get our hands dirty.\u003C/p>\n\u003Ch2 id=\"chaining-entities\">Chaining entities\u003C/h2>\n\u003Cp>Pretty much any well-known AI provider today is offering a web UI to chat with their model, but also to create custom agents,\nconnect some mainstream tools like your calendar, your emails, your source code or whatsoever.\u003C/p>\n\u003Cp>\u003Cstrong>By the way, quick parenthesis here as a kindly reminder that todayâ€™s free or individual plans offered by big AI companies\nfor an access to a web interface to chat and play around with models are well underpriced. That means, they\nbenefit from your daily usage of such tool (your data, the feedbacks you makeâ€¦).\u003C/strong>\u003C/p>\n\u003Cp>Disclaimer done. Now what brings Mastra if you can already create agents only? I continued my exploration by \u003Cstrong>creating a tool\u003C/strong>.\nA tool is a interface to and from another program. \u003Cstrong>Itâ€™s basically a side effect\u003C/strong>. There are input and output data to almost any\ntool, validated by a schema. I decided to create tools to interact with a backend of a todo list sandbox project, so that my agent\ncan manage for me this list.\u003C/p>\n\u003Cp>The first tool is to fetch all the remaining todos for the day, the second to mark one task as complete, and a last one to create\nnew elements in my list. Now that my tools are ready, I can connect them to my agent, and with a properly tuned system prompt, my agent would know\nexactly when to use those tools.\u003C/p>\n\u003Cp>And you get immediate results! ðŸ¤¯\u003C/p>\n\u003Cp>With Studio, itâ€™s super easy to test your tools independently, or together, interact with your agent and verify itâ€™s calling the appropriate\ntools, see the input and output of each call and so on!\u003C/p>\n\u003Cp>For complex tasks involving multiple tools or agents, you can create workflows. Workflows basically allow to have one tool (or agent) called\nafter another, which each step responsible for a defined task.\u003C/p>\n\u003Ch2 id=\"connect-infrastructure\">Connect infrastructure\u003C/h2>\n\u003Cp>You can add super-powers to your AI just by giving it some infrastructure resources. Within minutes you can give your agents a memory, that means\nthat your agent will remember previous messages in the conversation, to potentially fine-tune the upcoming answers. If you add another adapter,\nyou can get your agent to share a memory across several conversations.\u003C/p>\n\u003Cp>Overall many of the advanced features (e.g., memory, RAG) of Mastra require you connect a database, so make yourself a favor and write a small \u003Ccode>compose.yml\u003C/code> file\nso you can have a confined database ready to serve your local environnement.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"yaml\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">services\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">  db\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    image\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">postgres:16\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    ports\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      - \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"5432:5432\"\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"i-understood-rag-retrieval-augmented-generation\">I understood RAG (Retrieval-Augmented Generation)\u003C/h2>\n\u003Cp>Now that Iâ€™m completely trapped into the rabbit hole, Iâ€™m curious to test some of the unknown and most advanced features. I went back to the\ndocumentation, clicked a random tab in the side navigation panel and pickedâ€¦ RAG.\u003C/p>\n\u003Cp>I first read a bit of theory about this topic to understand what this is all about before testing it. I will try to make rapid explaination here:\nRAG uses non-specialist AI models to answer very specific questions using additionnal knowledge taken from resources like text documents.\nThe benefits of this technique is that your AI will be able to answer based on documents that have never been used to train public AI models, letâ€™s say\nfor example your product documentation.\u003C/p>\n\u003Cp>itâ€™s a multiple steps process that involves \u003Cstrong>parsing some documents\u003C/strong>, \u003Cstrong>splitting it\u003C/strong>, \u003Cstrong>embedding it\u003C/strong>, \u003Cstrong>storing vectors\u003C/strong> and \u003Cstrong>make them accessible\u003C/strong> to an LLM.\nIn other terms, that means:\u003C/p>\n\u003Cul>\n\u003Cli>Read some input documents with an appropriate parser (letâ€™s say your documentation in markdown)\u003C/li>\n\u003Cli>Make small chunks out of it (can be a group of words, a number of letter, a paragraphâ€¦)\u003C/li>\n\u003Cli>Transform chunks into vectors: thatâ€™s called embeddings\u003C/li>\n\u003Cli>Store the vector into a vector database (if you like postgres, there is a pgVector adapter)\u003C/li>\n\u003Cli>Make the database accessible to your agents\u003C/li>\n\u003C/ul>\n\u003Cp>I tried this with the documentation of an old project I work on, and the result felt exactly like the support chatbots you sometimes stumble across online. Except that\nmy agent felt way more talentedâ€¦ ðŸ˜œ\u003C/p>\n\u003Ch2 id=\"road-to-production\">Road to production\u003C/h2>\n\u003Cp>Now the obvious question that comes after days of experimenting with the codebase and Studio is: how do I host this thing and how do I connect it to my project?\nOnce more Mastra got you covered with a bunch of options to choose from:\u003C/p>\n\u003Cul>\n\u003Cli>Mastra Cloud (the commercial offer)\u003C/li>\n\u003Cli>Integration with another framework like Next.js or Astro\u003C/li>\n\u003Cli>Serverless deployment\u003C/li>\n\u003Cli>Custom server deployment\u003C/li>\n\u003C/ul>\n\u003Cp>About this last option: Mastra provides a ready to use HTTP API that allows to call each one of the tools, agents, workflows - and generally spoken entities - you created in\nyour Mastra codebase. That means you can from your mobile app for example with just one HTTP call talk to your custom agent. And to make things even simpler, if your\nfrontend codebase is in Typescript, they provide a client to simplify interactions with the server.\u003C/p>\n\u003Ch2 id=\"why-you-should-try\">Why you should try\u003C/h2>\n\u003Cp>If like me you donâ€™t want to lag behind the market and want to strengthen your AI understanding and skillset, itâ€™s a perfect sandbox. There is a whole bunch of\nuse-cases that emerge from the experiments you can do with Mastra. \u003Cstrong>Itâ€™s a fun way of learning\u003C/strong>.\u003C/p>\n\u003Cp>And if you accidentally create the perfect solution to a problem with this framework (you might), then you will be lucky enough to have a production ready solution,\nbacked by talented open-source contributors: the team behind Mastra is also responsible for the success of Gatsby, the frontend framework.\u003C/p>\n\u003Cp>\u003Cstrong>Stay tuned, Mastra 1.0 is going live in January 2026\u003C/strong>\u003C/p>",{"headings":130,"localImagePaths":156,"remoteImagePaths":157,"frontmatter":158,"imagePaths":160},[131,135,138,141,144,147,150,153],{"depth":132,"slug":133,"text":134},2,"what-caught-my-attention","What caught my attention",{"depth":132,"slug":136,"text":137},"getting-started-it-takes-literally-2-minutes","Getting started, it takes literally 2 minutes",{"depth":132,"slug":139,"text":140},"discovering-studio-ui","Discovering Studio UI",{"depth":132,"slug":142,"text":143},"chaining-entities","Chaining entities",{"depth":132,"slug":145,"text":146},"connect-infrastructure","Connect infrastructure",{"depth":132,"slug":148,"text":149},"i-understood-rag-retrieval-augmented-generation","I understood RAG (Retrieval-Augmented Generation)",{"depth":132,"slug":151,"text":152},"road-to-production","Road to production",{"depth":132,"slug":154,"text":155},"why-you-should-try","Why you should try",[],[],{"title":119,"cover":125,"date":159},["Date","2025-12-30T00:00:00.000Z"],[]]